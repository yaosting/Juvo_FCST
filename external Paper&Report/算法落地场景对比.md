| 算法       | 发布年 | 短时序预测 | 长时序预测 | 稀疏数据预测 |
|------------|--------|------------|------------|---------------|
| NHITS      | 2021   | 一般       | 优秀       | 一般          |
| NBEATS     | 2019   | 优秀       | 良好       | 一般          |
| PatchTST   | 2021   | 优秀       | 良好       | 良好          |
| TFT        | 2020   | 优秀       | 优秀       | 优秀          |
| Informer   | 2021   | 一般       | 优秀       | 一般          |
| TimesNet   | 2022   | 优秀       | 优秀       | 良好          |


## 1. NHITS
短时序预测：NHITS（N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting）专为处理层次和交叉时间序列设计，其在短时序预测上可能需要足够的数据来捕捉时间序列的模式，对于数据量较少的情况可能不是最优选择。
长时序预测：NHITS强于处理长期依赖问题，因为其层次结构设计有助于捕捉和预测长期趋势和周期性模式。
稀疏数据预测：对于稀疏数据，NHITS的性能可能取决于其能否通过其结构有效插值稀疏点。
## 2. N-BEATS
短时序预测：N-BEATS展现出卓越的短期预测性能，其纯前馈深度学习架构使其在捕捉短期依赖关系方面表现优异。
长时序预测：虽然主要设计用于短期预测，但N-BEATS通过堆叠多个模型和采用长期回溯窗口，也能处理一些长期预测问题。
稀疏数据预测：N-BEATS的表现可能受限于数据稀疏性，因为其模型性能高度依赖于足够的历史数据。
## 3. PatchTST
短时序预测：PatchTST是基于Transformer的时序处理方法，利用局部感知能力对短期依赖关系进行建模，对于短时序预测具有较好的性能。
长时序预测：对于长时序预测，PatchTST可以通过其自注意力机制捕获长距离依赖，但效果可能受限于序列长度和模型复杂度。
稀疏数据预测：由于Transformer架构的自注意力机制，PatchTST可能对稀疏数据具有一定的处理能力，但需要足够的训练数据支持。
## 4. TFT (Temporal Fusion Transformers)
短时序预测：TFT通过利用自注意力机制和多变量输入，对于短期时序预测表现良好，尤其是在处理复杂的、多变量的时间序列时。
长时序预测：TFT的设计使其能够处理长期依赖问题，其多头注意力机制有助于捕捉长期时间序列中的复杂模式。
稀疏数据预测：TFT通过其输入门和输出门的设计，能够有效处理稀疏数据，通过学习重要的时间点来提高预测准确性。
## 5. Informer
短时序预测：Informer专为长期时间序列预测设计，对于短期预测，它可能不如专门针对短期设计的模型高效。
长时序预测：Informer在长期预测方面表现出色，其独特的概率稀疏自注意力机制有效降低了计算复杂度，同时保持了对长距离依赖的机制
